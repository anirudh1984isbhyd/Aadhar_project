################################################################################################################
#	FUNCTION DEF
################################################################################################################

import re                                                        
import os 
import pandas as pd
import numpy as np
from pandas import ExcelWriter


import nltk
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer
from matplotlib import pyplot as plt
%matplotlib inline


active_dir = "/Users/anirudhsyal/Desktop/Hindu_news"
os.chdir (active_dir )

global count_proc
count_proc = 0
#################################################
def stopplaces():
    stop_places_f_name = "location_stop_words.txt"
    stop_places = list(open( stop_places_f_name, 'r'))
    lis_iterator = 0
    while lis_iterator <len(stop_places):
        stop_places[lis_iterator] = re.sub(r'\n', '', stop_places[lis_iterator] )
        stop_places[lis_iterator] = re.sub(r'[^a-zA-Z\s]', '', stop_places[lis_iterator] )
        stop_places[lis_iterator] = re.sub(r'\s\s+', ' ', stop_places[lis_iterator] )

        lis_iterator = lis_iterator + 1

    return stop_places

def clean_corpus_1(text): # read text and return a list of clean paragraphs
   
    import re                                                        
    import os 
    import pandas as pd
    
    #fname= input('Input corpus filename - e.g :/Users/anirudhsyal/Desktop/reviews.txt')
    #fname = "/Users/anirudhsyal/Desktop/nklrev_2.txt"
    #text = open(fname, 'r').readlines()        
    #text = [' I am happy & AT&T and AT &T working', ' I $ $40 is great']
    #print('no of pararagraphs in texts your file=', len(text), 'stored as ', type(text))
                                  
    
    if(len(text) == 0):
        text = 0
        return text
    
    replacement_patterns = [
     (r'won\'t', 'will not'),
     (r'can\'t', 'cannot'),
     (r'i\'m', 'i am'),
     (r'ain\'t', 'is not'),
     (r'(\w+)\'ll', '\g<1> will'),
     (r'(\w+)n\'t', '\g<1> not'),
     (r'(\w+)\'ve', '\g<1> have'),
     (r'(\w+)\'s', '\g<1> is'),
     (r'(\w+)\'re', '\g<1> are'),
     (r'(\w+)\'d', '\g<1> would')]


    # Subdivide text into paragraphs
    # convert to lower case
    # eliminate HTML type elements within '< > '
    # eliminate all ? , ! , ?? , #  type tokens from the paragrpah
    # input a space between a digit and a word boundary
    # do not tokenzie M.R or U.S.A  type tokens from the paragrpah - Keep them as is
    # eliminate words recurring after one another
    # remove dangling spaces, collapse one or more spaces into one space #
    # substitute known short forms with actual words from the defined list replacement patterns
    # remove remaining '  or " characters
    
    text = text.lower() 

    if 'aadhaar' in text or 'aadhar' in text or 'aaadhar' in text or 'aaadhaar' in text:
        text = re.sub(r'(<.*?>)', ' ', text)  
        text = re.sub(r'(\d+)(\-|:)(\d+)', ' ' , text)
        text = re.sub(r'(\&|\%|\$|\.|\?|!|#)(\s|\&|\.|\%|\$|\?|!|#)*(\1+)', r' \1 ' , text)  

        text = re.sub(r'(\%)', ' percent ' , text) 
        text = re.sub(r'(\$)', ' dollars ' , text)  

        text = re.sub(r'[^a-z0-9\s\.\'\"\-\_\&]', ' ',text)  
        text = re.sub(r'\s+[\-\&\_]', ' ', text)  

        text  = re.sub(r'(\d)([a-z]{2,})',r'\1 \2' , text )
        text  = re.sub(r'(\d)\s([a-z])([\s\.])(?![a-z])',r'\1\2\3' , text )  

        text = re.sub(r'\.\.+', '. ', text)
        text = re.sub(r'xa0',' ', text)

        
        text = re.sub(r'aadhar', 'aadhaar' , text)
        text = re.sub(r'aaadhar', 'aadhaar' , text)
        text = re.sub(r'aaadhaar', 'aadhaar' , text)
        text = re.sub(r'\s+pd(s)*\s+', 'public_distribution ' , text)
        text = re.sub(r'\s+',' ', text)  
        
        for pattern in replacement_patterns: 
            text = re.sub (pattern[0],  pattern[1] , text)

        text = re.sub(r'[^a-z0-9\s\.\&\_\-]', ' ', text)  
        text = re.sub(r'(\be)(\s|\-|\_)*(mail|watch|commerce|bay)', r'\1\3' , text)
        text = re.sub(r'(\bi)(\s|\-|\_)*(phone|watch|tunes)', r'\1\3' , text)
        text = re.sub(r'dbt', ' direct benefit transfer ' , text)
        text = re.sub(r'm(g)*nrega', ' nrega ' , text)
        text = re.sub(r'per cent', ' percent ' , text)
        text = re.sub(r'\.the', '. the' , text)
        text = re.sub(r'(database\.|\s*)(googletag)(\.cmd\.push|\.display)', ' ' , text)
        text = re.sub(r'adslotnativevideo', ' ' , text)
        text = re.sub(r'food\s+grain(s)*', 'foodgrains' , text)

    else:
        text = 0

    return text 

def clean_corpus_2 (text, stop_places): 
    text = text.strip()
    iter = True
    while iter:
        for junk in stop_places:
            if text.startswith(junk):
                text = text[len(junk):]
                iter = False
        iter = False
        
    return text

def clean_corpus_3(text,stop_words): 
    global count_proc
    count_proc = count_proc +1
    import re
    new_lis =text.split()
    word_list = []
    for word in new_lis:
        word = re.sub(r'\s+', '', word) 
        word = re.sub(r'^[^a-z0-9]+', '', word)
        word = re.sub(r'[^a-z0-9]+$', '', word)
        word = re.sub(r'\.com|\.in|\.net', '', word)
        word = re.sub(r'\.',"" , word)
        appen_d = True
    
        if word in stop_words or word.isdigit() or len(word)< 2:
            appen_d = False  
        elif re.findall(r'[\&\-\_]', word) or len(word) == 2:
            appen_d = True
        #elif word is 'apps':
        #    word = 'app'
        #elif word is 'iphones':
        #    word = 'iphone'   
        else: 
            word = lemmatize(word)

        if appen_d:
            word_list.append(word)
    
    text = ' '.join(word_list) 
    count_proc_all = [100, 400, 500, 1000, 1500, 2000, 2500, 3000]
    if count_proc in count_proc_all:
        print(count_proc)

    return text

def clean_corpus_4(text):
    repl_words_lis = ['accept','affect','generate','challenge','concern','consolidate',
                        'prescribe','absolute','alleged','apparent','careful','complete',
                         'digital','direct','elder','eventual','exact','final','formal',
                         'full','heavy','immediate','initial','direct','particular','physical',
                         'public','quick','recent','reported','simple','subsequent',
                         'successful','virtual','wide','unauthorise', 'unfortunate']
 
    new_lis =text.split()
    
    for i in range(0,len(new_lis)):
        for elem in repl_words_lis:
            if elem in new_lis[i]:
                new_lis[i] = elem

    text = ' '.join(new_lis)
    return text

def clean_corpus_5_replace_unigrams_with_bi_grams(text, bi_gram_lis,tri_gram_lis ):
    
    bi_gram_lis = bi_gram_lis
    tri_gram_lis  = tri_gram_lis
    new_lis =text.split()
    ret_list = []
    i=0
    while i < len(new_lis):
        if i == len(new_lis)-1:
            ret_list.append(new_lis[i])
            i = i+1
        else:
            query_bi = new_lis[i] + '_' + new_lis [i+1]
            if i <= len(new_lis) -3:
                query_tri = query_bi + '_' + new_lis [i+2]

                if query_tri in tri_gram_lis:
                    ret_list.append(query_tri)
                    i = i+3 
                elif query_bi in bi_gram_lis: 
                    ret_list.append(query_bi)
                    i = i+2
                else:
                    ret_list.append(new_lis[i])
                    i=i+1
            elif query_bi in bi_gram_lis:
                ret_list.append(query_bi)
                i = i+2
            else:
                ret_list.append(new_lis[i])
                i=i+1


    text = ' '.join(ret_list)  

    return text



def stopwords():  
# import NLTK stopwords and user defined stopwords if available and return a list 

    from nltk.corpus import stopwords
    #import pandas as pd
    #from pandas import ExcelWriter
    #import os 
    #import re
    # genereic stopwords found in NLTK
    stop_words = stopwords.words('english')
    # user defined stop words in text format handles both Y and N - add stopwords as text file only
    repeat_input = True
    while repeat_input: 
        #Response = input('Do you have a specefic stopwords text file - input  Y for yes or N for no ')
        Response = 'y'
        if Response.lower() == 'y':
            #my_stop_words_fname = input('Input user defined stop words in text file format')
            my_stop_words_fname = "more_stop_words.txt"
            my_stop_words = list(open( my_stop_words_fname, 'r'))
            for word in my_stop_words:
                stop_words.append(word)
            repeat_input = False
        if Response.lower() == 'n':
            repeat_input = False                

    lis_iterator = 0
    while lis_iterator <len(stop_words):
        stop_words[lis_iterator] = re.sub(r'\n', '', stop_words[lis_iterator] )
        stop_words[lis_iterator] = re.sub(r'[^a-zA-Z]', '', stop_words[lis_iterator] )
        lis_iterator = lis_iterator + 1
    
    new_append = ['said', 'although','though', 'actually', 'hi', 'hello',  'it', 'get' ,'k.k','function',
                  'commentsthe', 'said', 't.co', 'official', 'advocate',  'shyam', 'gupta', 'roy', 'kv', 'raja'
                  'commentshe', 'commentsmr', 'chidambaram', 'besides','ariyalur', 'devi', 'kurnool','minister dikshit'
                 'commentsa', 'dr', 'chandrachud ashok', 'today', 'chandra', 'kiran reddy','sp','anand','anil', 
                 'po', 'krishna rao', 'mv','ii', 'one', 'two', 'day', 'us']
    
    
    
    for new in new_append:
        stop_words.append(new)

    stop_words = list(set(stop_words))
    return stop_words

def part_of_speec_tag (treebank_tag):
    import nltk
    from nltk.corpus import wordnet
    from nltk import pos_tag
    from nltk.tokenize import word_tokenize

    if treebank_tag.startswith('J'):
        #print(wordnet.ADJ)
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        #print(wordnet.VERB)
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        #print(wordnet.NOUN)
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
       # print(wordnet.ADV)
        return wordnet.ADV
    else:
        return None # for easy if-statement  
        

def lemmatize(word):
    
    import nltk
    from nltk.stem.wordnet import WordNetLemmatizer
    from nltk.tokenize import word_tokenize

    lemmatizer = WordNetLemmatizer()
    word = word_tokenize(word)
    tagged = nltk.pos_tag(word)
    #print(tagged)
    
    for word, tag in tagged:
        wntag = part_of_speec_tag(tag)
        if wntag is None:
            word = lemmatizer.lemmatize(word) 
        else:
            word = lemmatizer.lemmatize(word, pos=wntag) 
    return word

def tokenizer_tf_idf(text):
    #import pandas as pd
    #import numpy as np
    #import re
    #import wordsegment
    #from wordsegment import load, segment
    #load()
    word_vector = text.split()
    
    return word_vector


def text_to_ngrams(paragraph_list,stop_words, n):  
    
    from nltk.util import ngrams
    import pandas as pd
    import numpy as np
    import os 
    
    stop_words = stop_words
    paragraph_list = paragraph_list
    no_of_paras = len(paragraph_list)
    n_grams = []
    para_iterator = 0
    for paragraphs in paragraph_list:
        #print(paragraphs)
        tokens = list(ngrams(paragraphs.split(), n))
        #print(tokens)
        para_iterator = para_iterator +1
        token_iterator = 0
        while token_iterator < len(tokens):
            ngram = ''
            new_lis = list(tokens[token_iterator])
            #print(para_iterator, token_iterator, new_lis )
            i = 0
            while i < n:
                new_lis[i] = clean_word(new_lis[i])  
                if len(new_lis[i]) == 0 or new_lis[i] in stop_words:
                    token_iterator=token_iterator+i+1
                    #print('stopwords', 'i is', i,'word is',  new_lis[i], 'tokeniterator is' ,token_iterator )
                    i = n
                    ngram = ''
                else:
                    if i==0:
                        ngram =  new_lis[i]
                    else:
                        ngram =  ngram + "-" + new_lis[i]
                    i=i+1
                    
            if len(ngram) >0:
                n_grams.append(dict( doct_no =para_iterator, ngram = ngram ))                
                token_iterator=token_iterator+1
    #print(n_grams)
    n_grams = pd.DataFrame(n_grams)
    
    n_grams['freq_in_para'] = n_grams.groupby(['doct_no', 'ngram' ])['ngram'].transform('count')
    n_grams_temp = n_grams.copy(deep=True)
    n_grams_temp.drop_duplicates(inplace = True)

    n_grams_temp['presence_in_docs'] = n_grams_temp.groupby(['ngram' ])['doct_no'].transform('count')
    n_grams_temp.drop(['freq_in_para', 'doct_no' ], axis = 1, inplace = True)
    n_grams_temp.drop_duplicates(inplace = True)
    n_grams = pd.merge(n_grams,n_grams_temp, on = 'ngram')
    
    if n ==2:
        n_grams_temp = n_grams[n_grams['presence_in_docs']>=4]
        n_grams_temp = n_grams_temp[n_grams_temp['freq_in_para']>=2]
   
    if n >2:
        n_grams_temp = n_grams[n_grams['presence_in_docs']>=4]
        n_grams_temp = n_grams_temp[n_grams_temp['freq_in_para']>1]

    g = n_grams_temp.groupby(['doct_no'])
    n_grams = []
    col_name =str(n) + '_'+ 'grams'
    for name, group in g:
        s = group.ngram.tolist()
        n_grams.append(dict(doct_no =name,lis =  s))
    n_grams = pd.DataFrame(n_grams)
    n_grams.rename(columns={'lis': col_name}, inplace=True)

    return n_grams

def final_tokenzied_list(df, colname, ngrams):
    stop_words = stopwords()
    paragraph_list = df[colname].tolist()
    df['tokens'] = df[colname].map(tokenizer_tf_idf)
    
    n_grams=2
    while n_grams < ngrams+1:
        col_name =str(n_grams) + '_'+ 'grams'
        n_gram_df = text_to_ngrams(paragraph_list,stop_words,n_grams)
        df = pd.merge(df,n_gram_df, on = 'doct_no',how='left')
        df['tokens'] = np.where(df[col_name].isnull(), df['tokens'], df['tokens'] + df[col_name])
        df.drop([col_name], axis = 1, inplace = True)
        n_grams = n_grams +1
    
    return df

def count_most_common(df,tok_col_name, top_n_counts):
    from collections import Counter
    def count_a(token_list):
        from collections import Counter
        counter = Counter(token_list)
        counter_tuple = counter.most_common(top_n_counts)
        return_list = []
        for i in range (0,top_n_counts):
            return_list.append(counter_tuple[i][0])
        return return_list
    n_count_col_name = 'top'+ '_' + str(top_n_counts)+'_'+'counts'
    df[n_count_col_name]= df[tok_col_name].map(count_a)
    return df


def TF_IDF(df,colname, min_df, ngram_range_tuple):
    from sklearn.feature_extraction.text import TfidfVectorizer
    #a = list(df['text'])
    a = list(df[colname])

    vectorizer = TfidfVectorizer(min_df=min_df, max_features=10000, tokenizer=tokenizer_tf_idf,stop_words =  stopwords(), ngram_range = ngram_range_tuple)
    vz = vectorizer.fit_transform(a)
    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
    tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')
    tfidf.columns = ['tfidf']
    #name = str(min_df)+'_'+ 'tfidf'+'.'+ 'xlsx'
    #writer_orig = pd.ExcelWriter(name)
    #tfidf.to_excel(writer_orig, sheet_name='tfidf')
    #writer_orig.save()
    print(len(tfidf))
    print(tfidf.head())
    return tfidf

def clean_non_news_body_cols(text): # read text and return a list of clean paragraphs
   
    import re                                                        
    
    #fname= input('Input corpus filename - e.g :/Users/anirudhsyal/Desktop/reviews.txt')
    #fname = "/Users/anirudhsyal/Desktop/nklrev_2.txt"
    #text = open(fname, 'r').readlines()        
    #text = [' I am happy & AT&T and AT &T working', ' I $ $40 is great']
    #print('no of pararagraphs in texts your file=', len(text), 'stored as ', type(text))
                                  
    
    if(len(text) == 0):
        text = 0
        return text
    
    replacement_patterns = [
     (r'won\'t', 'will not'),
     (r'can\'t', 'cannot'),
     (r'i\'m', 'i am'),
     (r'ain\'t', 'is not'),
     (r'(\w+)\'ll', '\g<1> will'),
     (r'(\w+)n\'t', '\g<1> not'),
     (r'(\w+)\'ve', '\g<1> have'),
     (r'(\w+)\'s', '\g<1> is'),
     (r'(\w+)\'re', '\g<1> are'),
     (r'(\w+)\'d', '\g<1> would')]

    text = text.lower() 

 
    text = re.sub(r'(<.*?>)', ' ', text)  
    text = re.sub(r'(\&|\%|\$|\.|\?|!|#)(\s|\&|\.|\%|\$|\?|!|#)*(\1+)', r' \1 ' , text)  
    text = re.sub(r'(\%)', ' percent ' , text) 
    text = re.sub(r'(\$)', ' dollars ' , text)  

    text = re.sub(r'[^a-z0-9\s\.\'\"\-\_\&]', ' ',text)  
    text = re.sub(r'\s+[\-\&\_]', ' ', text)  

    text  = re.sub(r'(\d)([a-z]{2,})',r'\1 \2' , text )
    text  = re.sub(r'(\d)\s([a-z])([\s\.])(?![a-z])',r'\1\2\3' , text )  

    text = re.sub(r'\.\.+', '. ', text)
    text = re.sub(r'xa0',' ', text)

    text = re.sub(r'aadhar', 'aadhaar' , text)
    text = re.sub(r'\s+',' ', text)  

    for pattern in replacement_patterns: 
        text = re.sub (pattern[0],  pattern[1] , text)

    text = re.sub(r'[^a-z0-9\s\.\&\_\-]', ' ', text)  
    text = re.sub(r'(\be)(\s|\-|\_)*(mail|watch|commerce|bay)', r'\1\3' , text)
    text = re.sub(r'(\bi)(\s|\-|\_)*(phone|watch|tunes)', r'\1\3' , text)
    text = re.sub(r'dbt', ' direct benefit transfer ' , text)
    text = re.sub(r'mgnrega', ' nrega ' , text)
    text = re.sub(r'per cent', ' percent ' , text)
    text = re.sub(r'\.the', '. the' , text)
    text = re.sub(r'(database\.|\s*)(googletag)(\.cmd\.push|\.display)', ' ' , text)
    text = re.sub(r'adslotnativevideo', ' ' , text)


    return text 

def count_token(tok_list, token):
    from collections import Counter
    counter = Counter(tok_list)
    counter_out = counter[token]
    return counter_out

def clean_check (text):
    text = re.sub(r'[^a-z]', ' ',text)  
    text = re.sub(r'\s\s+', ' ', text)  
    return text

  
def clear_more_duplicates(df,on_col):

    df.reset_index(inplace=True,drop=True)
    df['index1'] = df.index
    sq = pd.DataFrame(df[on_col].copy(deep=True))

    sq.drop_duplicates(inplace=True)
    sq['index1'] = sq.index

    merged = pd.merge(sq,df, on=['index1', on_col])
    merged.drop(['index1'], axis = 1, inplace = True)
    merged.reset_index(inplace=True,drop=True)
    merged['doct_no'] = range(len(merged))
    merged['doct_no'] = merged['doct_no'] +1
    return merged

###############################################################################################################################
#	RUNNING CMDS
###############################################################################################################################
stop_places = stopplaces()
stop_words = stopwords()
news_df = pd.read_pickle('hindu_df.pkl')
news_df.NewsBody = news_df.NewsBody.astype(str)
news_df.DateTime = news_df.DateTime.astype(str)
news_df.Heading = news_df.Heading.astype(str)
news_df.drop_duplicates(inplace = True)
print(len(news_df))

news_df['NewsBody'] = news_df['NewsBody'].apply(clean_corpus_1)
news_df = news_df[news_df['NewsBody']!= 0]
news_df.rename(columns = {'NewsBody':'text'}, inplace = True)
news_df['original_text'] = news_df['text']
news_df['DateTime'] = news_df['DateTime'].apply(clean_non_news_body_cols)
news_df['Heading'] = news_df['Heading'].apply(clean_non_news_body_cols)


#########
news_df['text'] = news_df['text'].apply(clean_corpus_2,stop_places = stop_places )
news_df['doct_no'] = range(len(news_df))
news_df['doct_no'] = news_df['doct_no'] +1
news_df.head()


#########

news_df['text'] = news_df.text.astype(str)
news_df['text'] = news_df['text'].apply(clean_corpus_3,stop_words = stop_words)


#########
news_df['text'] = news_df['text'].apply(clean_corpus_4)
#########
#join_bi_grams and tri_grams##################################

bi_gram_lis = pd.read_excel('bigrams_1.xlsx').Bigrams.tolist()
tri_gram_lis = pd.read_excel('trigrams_1.xlsx').Trigrams.tolist()

news_df['text'] = news_df['text'].apply(clean_corpus_5_replace_unigrams_with_bi_grams, 
                                            bi_gram_lis = bi_gram_lis, tri_gram_lis = tri_gram_lis)

bi_gram_lis = pd.read_excel('bigrams_2.xlsx').Bigrams.tolist()
tri_gram_lis = pd.read_excel('trigrams_2.xlsx').Trigrams.tolist()

news_df['text'] = news_df['text'].apply(clean_corpus_5_replace_unigrams_with_bi_grams, 
                                            bi_gram_lis = bi_gram_lis, tri_gram_lis = tri_gram_lis)

######################

# more cleaning check for aadhaar token count in each paragraph

news_df['new_text'] = news_df['text'].map(clean_check)
news_df['new_tokens'] = news_df['new_text'].map(tokenizer_tf_idf)
news_df['aadhaar_count'] = news_df['new_tokens'].apply(count_token, token = 'aadhaar')
len(news_df[news_df['aadhaar_count']== 1])
news_df['aadhaar_count'].hist(bins=100)


#########

news_df = count_most_common(news_df,'new_tokens', 1)

def col_to_str (top_count_col):
    for word in top_count_col:
        return word
    
news_df['new_col'] = news_df['top_1_counts'].apply(col_to_str )

d_stop =pd.read_excel('hindu_non_relevant_entries.xlsx')
d_stop.entires = d_stop.entires.astype(str)

def remove_more_junk_hindu_posts (text, compare_list):
    if text in compare_list:
        return 0
    else:
        return 1
    
news_df['compare'] =  news_df['new_col'].apply(remove_more_junk_hindu_posts,compare_list = d_stop.entires.tolist() )
print(len(news_df[news_df['compare']==0]))
print('original', len(news_df))
news_df = news_df[news_df['compare']==1]
print('final', len(news_df))


news_df.drop(['new_text', 'new_tokens' , 'top_1_counts', 'new_col','compare' ], axis = 1, inplace = True)
news_df.head()


news_df.to_pickle('final_clean_hindu_corpus.pkl')

## generate TF_IDF

tfidf = TF_IDF(news_df,'text', 30, (1,1))
writer_orig = pd.ExcelWriter('hindu_tfidf_011.xlsx')
tfidf.to_excel(writer_orig, sheet_name='hindu_tfidf_010')
writer_orig.save()
