################################################################################################################
#	FUNCTION DEF
################################################################################################################

from bs4 import BeautifulSoup
import pandas as pd
import requests
import re

def chk_for_toi_aadhaar_articles(link,aadhar_articles_links):
    response=requests.get(link)
    strings=re.findall('(\".*?\")',response.text)
    for strs in strings:
        if 'aadhaar' in strs:
            print(strs)
            aadhar_articles_links.append(strs)
        elif 'aadhar' in strs:
            print(strs)
            aadhar_articles_links.append(strs)
    return(aadhar_articles_links)



def get_toi_archive_links():
    days_in_month_dict = {"1": 31, "2": 28,"3": 31, "4": 30,"5": 31, "6": 30,"7": 31, "8": 31,
                      "9": 30, "10": 31,"11": 30, "12": 31}

    counter_in_year_dict = {"2017": 42736, "2016": 42370,"2015": 42005, "2014": 41640,"2013": 41275, "2012": 40909,"2011": 40544, "2010": 40179,
                      "2009": 39814}

    archive_url=[]
    url_part1="https://timesofindia.indiatimes.com/"
    for year in range(2017,2008,-1):
        counter=counter_in_year_dict[str(year)]
        for month in range(1,13):
            for days in range(1,days_in_month_dict[str(month)]+1):
                url=url_part1+str(year)+"/"+str(month)+"/"+str(days)+"/archivelist/year-"+str(year)+",month-"+str(month)+",starttime-"+str(counter)+".cms"
                print(url)
                archive_url.append(url)
                counter=counter+1
    return archive_url

def scrape_toi_news_articles(toi_news,aadhar_articles_links):
    for i in range(len(aadhar_articles_links)):
        aadhar_articles_links[i]=aadhar_articles_links[i].strip('\"')
        if(new_aadhar_links[i].find("http")==-1):
            new_aadhar_links[i]="http://timesofindia.indiatimes.com/"+new_aadhar_links[i] 


        if(new_aadhar_links[i].find("\"")!=-1):
            new_aadhar_links[i]=new_aadhar_links[i].replace("\"",'') 

        response2=requests.get(aadhar_articles_links[i])
        if(response2.content):
            soup = BeautifulSoup(response2.content, 'html.parser')


            source_link=aadhar_articles_links[i]

        #print(source_link)
            news_body_tag=soup.find('div',attrs={'class':'Normal'})
            if(news_body_tag):
                news_body=news_body_tag.text
                heading=soup.find('h1')
                if heading is not None:
                    heading=heading.text
                else:
                    heading="No Content"
                    print("No Content HEADING")
                    print(source_link)

                    #print(heading)

                datetime_tag=soup.find_all('span',attrs={'class':'time_cptn'})   
                for d in datetime_tag:
                    d1=d.find_all("span")
                    for d2 in d1:
                        d3=d2.find("span")
                        if(d3 is None):
                            date=d2.text
                        else:
                            date="No Content"
                            print("No Content:Datetime")
                            print(source_link)

                            #print(date)


                toi_news.append(dict(Heading=heading,DateTime=date,SourceLink="TOI",NewsBody=news_body))
            else:
                news_body="No Content"
                print("No Content :BODY")
                print(source_link)

    return toi_news

###############################################################################################################################
#	RUNNING CMDS
###############################################################################################################################



aadhar_articles_links=[]
archive_links=[]
archive_links=get_toi_archive_links()
for i in range(len(archive_links)):
    aadhar_articles_links=chk_for_toi_aadhaar_articles(archive_links[i],aadhar_articles_links)
toi_news=[]
toi_news=scrape_toi_news_articles(toi_news,new_aadhar_links)


