
###############################################################################################################################
#	RUNNING CMDS
###############################################################################################################################

news_df = pd.read_pickle('final_clean_hindu_corpus.pkl')

news_df = clear_more_duplicates(news_df, 'text')

## 30 march 2017
## topic modeling 
## LDA
from sklearn.feature_extraction.text import CountVectorizer
cvectorizer = CountVectorizer(max_df = 0.4, min_df=0.01, max_features=10000, tokenizer=tokenizer_tf_idf,
                              stop_words= stopwords(), ngram_range=(1,1))
cvz = cvectorizer.fit_transform(news_df['text'])
vocab = cvectorizer.get_feature_names()

import lda
import logging
logging.getLogger("lda").setLevel(logging.WARNING)

print('############################################')

count_topics = [20, 21, 22, 23]
for n_topics in count_topics:
    name = './pyldadavis_v2' + str(n_topics) + '.html'
    
    print('n_topics = ', n_topics )
    n_iter = 2000
    lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)
    X_topics = lda_model.fit_transform(cvz)
    n_top_words = 20
    topic_summaries = []

    topic_word = lda_model.topic_word_  # get the topic words
    vocab = cvectorizer.get_feature_names()

    #for i, topic_dist in enumerate(topic_word):
     #   topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
      #  topic_summaries.append(' '.join(topic_words))
       # print('Topic {}: {}'.format(i, ' '.join(topic_words)))


    from sklearn.manifold import TSNE
    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
    tsne_lda = tsne_model.fit_transform(X_topics)
    doc_topic = lda_model.doc_topic_
    lda_keys = []

    for i, tweet in enumerate(news_df['text']):
        lda_keys += [doc_topic[i].argmax()]


    lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])
    lda_df['doct_no'] = news_df['doct_no']
    lda_df['topic'] = lda_keys
    lda_df['text'] = news_df['text']


    #Preparing Lda_df
    news_df['tokens'] = news_df['text'].map(tokenizer_tf_idf)
    lda_df['len_docs'] = news_df['tokens'].map(len)

    def prepareLDAData():
        data = {
            'vocab': vocab,
            'doc_topic_dists': lda_model.doc_topic_,
            'doc_lengths': list(lda_df['len_docs']),
            'term_frequency':cvectorizer.vocabulary_,
            'topic_term_dists': lda_model.components_
        } 
        return data
    ldadata = prepareLDAData()

    import pyLDAvis

    pyLDAvis.enable_notebook()
    prepared_data = pyLDAvis.prepare(**ldadata)
    name = './pyldadavis_' + str(n_topics) + '.html'
    pyLDAvis.save_html(prepared_data,name)


    print('############################################')